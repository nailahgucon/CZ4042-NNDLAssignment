{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NFVxWZGJxprU"
      },
      "source": [
        "# CS4001/4042 Assignment 1, Part B, Q2\n",
        "In Question B1, we used the Category Embedding model. This creates a feedforward neural network in which the categorical features get learnable embeddings. In this question, we will make use of a library called Pytorch-WideDeep. This library makes it easy to work with multimodal deep-learning problems combining images, text, and tables. We will just be utilizing the deeptabular component of this library through the TabMlp network:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "EycCozG06Duu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f985780d-8fa0-4743-d930-df12e84436b5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pytorch-widedeep\n",
            "  Downloading pytorch_widedeep-1.3.2-py3-none-any.whl (21.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.8/21.8 MB\u001b[0m \u001b[31m72.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas>=1.3.5 in /usr/local/lib/python3.10/dist-packages (from pytorch-widedeep) (1.5.3)\n",
            "Requirement already satisfied: numpy>=1.21.6 in /usr/local/lib/python3.10/dist-packages (from pytorch-widedeep) (1.23.5)\n",
            "Requirement already satisfied: scipy>=1.7.3 in /usr/local/lib/python3.10/dist-packages (from pytorch-widedeep) (1.11.3)\n",
            "Requirement already satisfied: scikit-learn>=1.0.2 in /usr/local/lib/python3.10/dist-packages (from pytorch-widedeep) (1.2.2)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (from pytorch-widedeep) (4.3.2)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (from pytorch-widedeep) (3.6.1)\n",
            "Requirement already satisfied: opencv-contrib-python in /usr/local/lib/python3.10/dist-packages (from pytorch-widedeep) (4.8.0.76)\n",
            "Requirement already satisfied: imutils in /usr/local/lib/python3.10/dist-packages (from pytorch-widedeep) (0.5.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from pytorch-widedeep) (4.66.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from pytorch-widedeep) (2.0.1+cu118)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from pytorch-widedeep) (0.15.2+cu118)\n",
            "Collecting einops (from pytorch-widedeep)\n",
            "  Downloading einops-0.7.0-py3-none-any.whl (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.6/44.6 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from pytorch-widedeep) (1.15.0)\n",
            "Collecting torchmetrics (from pytorch-widedeep)\n",
            "  Downloading torchmetrics-1.2.0-py3-none-any.whl (805 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m805.2/805.2 kB\u001b[0m \u001b[31m58.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyarrow in /usr/local/lib/python3.10/dist-packages (from pytorch-widedeep) (9.0.0)\n",
            "Collecting fastparquet>=0.8.1 (from pytorch-widedeep)\n",
            "  Downloading fastparquet-2023.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m81.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting cramjam>=2.3 (from fastparquet>=0.8.1->pytorch-widedeep)\n",
            "  Downloading cramjam-2.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m78.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from fastparquet>=0.8.1->pytorch-widedeep) (2023.6.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from fastparquet>=0.8.1->pytorch-widedeep) (23.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.3.5->pytorch-widedeep) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.3.5->pytorch-widedeep) (2023.3.post1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0.2->pytorch-widedeep) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0.2->pytorch-widedeep) (3.2.0)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim->pytorch-widedeep) (6.4.0)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy->pytorch-widedeep) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy->pytorch-widedeep) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy->pytorch-widedeep) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy->pytorch-widedeep) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy->pytorch-widedeep) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy->pytorch-widedeep) (8.1.12)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy->pytorch-widedeep) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy->pytorch-widedeep) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy->pytorch-widedeep) (2.0.10)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy->pytorch-widedeep) (0.9.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy->pytorch-widedeep) (0.10.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy->pytorch-widedeep) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy->pytorch-widedeep) (1.10.13)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy->pytorch-widedeep) (3.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy->pytorch-widedeep) (67.7.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy->pytorch-widedeep) (3.3.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->pytorch-widedeep) (3.12.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->pytorch-widedeep) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->pytorch-widedeep) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->pytorch-widedeep) (3.1)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch->pytorch-widedeep) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->pytorch-widedeep) (3.27.6)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->pytorch-widedeep) (17.0.2)\n",
            "Collecting lightning-utilities>=0.8.0 (from torchmetrics->pytorch-widedeep)\n",
            "  Downloading lightning_utilities-0.9.0-py3-none-any.whl (23 kB)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->pytorch-widedeep) (9.4.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas>=1.3.5->pytorch-widedeep) (1.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy->pytorch-widedeep) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy->pytorch-widedeep) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy->pytorch-widedeep) (2.0.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy->pytorch-widedeep) (2023.7.22)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy->pytorch-widedeep) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy->pytorch-widedeep) (0.1.3)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy->pytorch-widedeep) (8.1.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy->pytorch-widedeep) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->pytorch-widedeep) (1.3.0)\n",
            "Installing collected packages: lightning-utilities, einops, cramjam, fastparquet, torchmetrics, pytorch-widedeep\n",
            "Successfully installed cramjam-2.7.0 einops-0.7.0 fastparquet-2023.8.0 lightning-utilities-0.9.0 pytorch-widedeep-1.3.2 torchmetrics-1.2.0\n"
          ]
        }
      ],
      "source": [
        "!pip install pytorch-widedeep"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "lq0elU0J53Yo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ec00d62-cbba-4ede-a89d-8d1dfab2e393"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<frozen importlib._bootstrap>:914: ImportWarning: APICoreClientInfoImportHook.find_spec() not found; falling back to find_module()\n",
            "<frozen importlib._bootstrap>:914: ImportWarning: _PyDriveImportHook.find_spec() not found; falling back to find_module()\n",
            "<frozen importlib._bootstrap>:914: ImportWarning: _OpenCVImportHook.find_spec() not found; falling back to find_module()\n",
            "<frozen importlib._bootstrap>:914: ImportWarning: _BokehImportHook.find_spec() not found; falling back to find_module()\n",
            "<frozen importlib._bootstrap>:914: ImportWarning: _AltairImportHook.find_spec() not found; falling back to find_module()\n"
          ]
        }
      ],
      "source": [
        "SEED = 42\n",
        "\n",
        "import os\n",
        "\n",
        "import random\n",
        "random.seed(SEED)\n",
        "\n",
        "import numpy as np\n",
        "np.random.seed(SEED)\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "from pytorch_widedeep.preprocessing import TabPreprocessor\n",
        "from pytorch_widedeep.models import TabMlp, WideDeep\n",
        "from pytorch_widedeep import Trainer\n",
        "from pytorch_widedeep.metrics import R2Score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aU3xdVpwzuLx"
      },
      "source": [
        ">Divide the dataset (‘hdb_price_prediction.csv’) into train and test sets by using entries from the year 2020 and before as training data, and entries from 2021 and after as the test data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "_oYG6lNIh7Mp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7703eb98-21ca-4e32-abc9-57d7190a204e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ],
      "source": [
        "df = pd.read_csv('hdb_price_prediction.csv')\n",
        "\n",
        "# TODO: Enter your code here\n",
        "# year 2020 and before as training data\n",
        "df_train = df[df['year'] <= 2020]\n",
        "# entries from 2021 and after as the test data\n",
        "df_test = df[df['year'] >= 2021]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m_q9PoR50JAA"
      },
      "source": [
        ">Refer to the documentation of Pytorch-WideDeep and perform the following tasks:\n",
        "https://pytorch-widedeep.readthedocs.io/en/latest/index.html\n",
        "* Use [**TabPreprocessor**](https://pytorch-widedeep.readthedocs.io/en/latest/examples/01_preprocessors_and_utils.html#2-tabpreprocessor) to create the deeptabular component using the continuous\n",
        "features and the categorical features. Use this component to transform the training dataset.\n",
        "* Create the [**TabMlp**](https://pytorch-widedeep.readthedocs.io/en/latest/pytorch-widedeep/model_components.html#pytorch_widedeep.models.tabular.mlp.tab_mlp.TabMlp) model with 2 linear layers in the MLP, with 200 and 100 neurons respectively.\n",
        "* Create a [**Trainer**](https://pytorch-widedeep.readthedocs.io/en/latest/pytorch-widedeep/trainer.html#pytorch_widedeep.training.Trainer) for the training of the created TabMlp model with the root mean squared error (RMSE) cost function. Train the model for 100 epochs using this trainer, keeping a batch size of 64. (Note: set the *num_workers* parameter to 0.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "ZBY1iqUXtYWn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0ec17159-4f70-4a23-dc77-47d3890a8116"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n",
            "/usr/local/lib/python3.10/dist-packages/pytorch_widedeep/preprocessing/tab_preprocessor.py:334: UserWarning: Continuous columns will not be normalised\n",
            "  warnings.warn(\"Continuous columns will not be normalised\")\n",
            "epoch 1: 100%|██████████| 1366/1366 [00:22<00:00, 60.98it/s, loss=2.31e+5, metrics={'r2': -2.2724}]\n",
            "epoch 2: 100%|██████████| 1366/1366 [00:18<00:00, 73.07it/s, loss=9.59e+4, metrics={'r2': 0.5559}] \n",
            "epoch 3: 100%|██████████| 1366/1366 [00:11<00:00, 116.28it/s, loss=8.25e+4, metrics={'r2': 0.6865}]\n",
            "epoch 4: 100%|██████████| 1366/1366 [00:12<00:00, 113.81it/s, loss=7.71e+4, metrics={'r2': 0.7314}]\n",
            "epoch 5: 100%|██████████| 1366/1366 [00:11<00:00, 115.10it/s, loss=7.41e+4, metrics={'r2': 0.7538}]\n",
            "epoch 6: 100%|██████████| 1366/1366 [00:11<00:00, 118.01it/s, loss=7.22e+4, metrics={'r2': 0.7674}]\n",
            "epoch 7: 100%|██████████| 1366/1366 [00:11<00:00, 122.09it/s, loss=7.09e+4, metrics={'r2': 0.7759}]\n",
            "epoch 8: 100%|██████████| 1366/1366 [00:12<00:00, 109.88it/s, loss=6.94e+4, metrics={'r2': 0.786}]\n",
            "epoch 9: 100%|██████████| 1366/1366 [00:12<00:00, 112.29it/s, loss=6.87e+4, metrics={'r2': 0.7905}]\n",
            "epoch 10: 100%|██████████| 1366/1366 [00:11<00:00, 114.50it/s, loss=6.79e+4, metrics={'r2': 0.7944}]\n",
            "epoch 11: 100%|██████████| 1366/1366 [00:11<00:00, 114.87it/s, loss=6.75e+4, metrics={'r2': 0.7969}]\n",
            "epoch 12: 100%|██████████| 1366/1366 [00:11<00:00, 117.22it/s, loss=6.69e+4, metrics={'r2': 0.8006}]\n",
            "epoch 13: 100%|██████████| 1366/1366 [00:11<00:00, 115.33it/s, loss=6.66e+4, metrics={'r2': 0.8021}]\n",
            "epoch 14: 100%|██████████| 1366/1366 [00:11<00:00, 114.25it/s, loss=6.61e+4, metrics={'r2': 0.8047}]\n",
            "epoch 15: 100%|██████████| 1366/1366 [00:12<00:00, 113.74it/s, loss=6.59e+4, metrics={'r2': 0.8063}]\n",
            "epoch 16: 100%|██████████| 1366/1366 [00:11<00:00, 115.85it/s, loss=6.54e+4, metrics={'r2': 0.8086}]\n",
            "epoch 17: 100%|██████████| 1366/1366 [00:11<00:00, 115.54it/s, loss=6.51e+4, metrics={'r2': 0.8105}]\n",
            "epoch 18: 100%|██████████| 1366/1366 [00:11<00:00, 118.73it/s, loss=6.49e+4, metrics={'r2': 0.8116}]\n",
            "epoch 19: 100%|██████████| 1366/1366 [00:11<00:00, 114.86it/s, loss=6.49e+4, metrics={'r2': 0.8112}]\n",
            "epoch 20: 100%|██████████| 1366/1366 [00:12<00:00, 113.67it/s, loss=6.44e+4, metrics={'r2': 0.8144}]\n",
            "epoch 21: 100%|██████████| 1366/1366 [00:11<00:00, 113.95it/s, loss=6.45e+4, metrics={'r2': 0.8135}]\n",
            "epoch 22: 100%|██████████| 1366/1366 [00:12<00:00, 112.69it/s, loss=6.42e+4, metrics={'r2': 0.8152}]\n",
            "epoch 23: 100%|██████████| 1366/1366 [00:11<00:00, 117.56it/s, loss=6.41e+4, metrics={'r2': 0.8157}]\n",
            "epoch 24: 100%|██████████| 1366/1366 [00:11<00:00, 115.35it/s, loss=6.4e+4, metrics={'r2': 0.8161}]\n",
            "epoch 25: 100%|██████████| 1366/1366 [00:12<00:00, 113.38it/s, loss=6.38e+4, metrics={'r2': 0.8174}]\n",
            "epoch 26: 100%|██████████| 1366/1366 [00:12<00:00, 112.94it/s, loss=6.36e+4, metrics={'r2': 0.8184}]\n",
            "epoch 27: 100%|██████████| 1366/1366 [00:12<00:00, 110.37it/s, loss=6.39e+4, metrics={'r2': 0.8163}]\n",
            "epoch 28: 100%|██████████| 1366/1366 [00:11<00:00, 117.37it/s, loss=6.34e+4, metrics={'r2': 0.8194}]\n",
            "epoch 29: 100%|██████████| 1366/1366 [00:11<00:00, 115.30it/s, loss=6.32e+4, metrics={'r2': 0.8209}]\n",
            "epoch 30: 100%|██████████| 1366/1366 [00:11<00:00, 114.30it/s, loss=6.31e+4, metrics={'r2': 0.8213}]\n",
            "epoch 31: 100%|██████████| 1366/1366 [00:11<00:00, 114.59it/s, loss=6.3e+4, metrics={'r2': 0.8214}]\n",
            "epoch 32: 100%|██████████| 1366/1366 [00:11<00:00, 115.25it/s, loss=6.26e+4, metrics={'r2': 0.8234}]\n",
            "epoch 33: 100%|██████████| 1366/1366 [00:11<00:00, 117.95it/s, loss=6.26e+4, metrics={'r2': 0.824}]\n",
            "epoch 34: 100%|██████████| 1366/1366 [00:11<00:00, 116.59it/s, loss=6.26e+4, metrics={'r2': 0.8236}]\n",
            "epoch 35: 100%|██████████| 1366/1366 [00:12<00:00, 113.38it/s, loss=6.24e+4, metrics={'r2': 0.8248}]\n",
            "epoch 36: 100%|██████████| 1366/1366 [00:12<00:00, 113.43it/s, loss=6.23e+4, metrics={'r2': 0.8252}]\n",
            "epoch 37: 100%|██████████| 1366/1366 [00:11<00:00, 114.69it/s, loss=6.2e+4, metrics={'r2': 0.8267}]\n",
            "epoch 38: 100%|██████████| 1366/1366 [00:11<00:00, 117.80it/s, loss=6.22e+4, metrics={'r2': 0.8257}]\n",
            "epoch 39: 100%|██████████| 1366/1366 [00:12<00:00, 113.49it/s, loss=6.21e+4, metrics={'r2': 0.8257}]\n",
            "epoch 40: 100%|██████████| 1366/1366 [00:12<00:00, 112.87it/s, loss=6.18e+4, metrics={'r2': 0.8279}]\n",
            "epoch 41: 100%|██████████| 1366/1366 [00:11<00:00, 114.88it/s, loss=6.18e+4, metrics={'r2': 0.8275}]\n",
            "epoch 42: 100%|██████████| 1366/1366 [00:11<00:00, 116.14it/s, loss=6.15e+4, metrics={'r2': 0.8291}]\n",
            "epoch 43: 100%|██████████| 1366/1366 [00:11<00:00, 116.10it/s, loss=6.14e+4, metrics={'r2': 0.8293}]\n",
            "epoch 44: 100%|██████████| 1366/1366 [00:11<00:00, 114.34it/s, loss=6.12e+4, metrics={'r2': 0.8307}]\n",
            "epoch 45: 100%|██████████| 1366/1366 [00:11<00:00, 116.30it/s, loss=6.11e+4, metrics={'r2': 0.831}]\n",
            "epoch 46: 100%|██████████| 1366/1366 [00:12<00:00, 112.36it/s, loss=6.09e+4, metrics={'r2': 0.8322}]\n",
            "epoch 47: 100%|██████████| 1366/1366 [00:12<00:00, 113.56it/s, loss=6.08e+4, metrics={'r2': 0.8328}]\n",
            "epoch 48: 100%|██████████| 1366/1366 [00:11<00:00, 114.36it/s, loss=6.08e+4, metrics={'r2': 0.8328}]\n",
            "epoch 49: 100%|██████████| 1366/1366 [00:12<00:00, 112.44it/s, loss=6.08e+4, metrics={'r2': 0.8327}]\n",
            "epoch 50: 100%|██████████| 1366/1366 [00:12<00:00, 112.44it/s, loss=6.04e+4, metrics={'r2': 0.8351}]\n",
            "epoch 51: 100%|██████████| 1366/1366 [00:12<00:00, 108.95it/s, loss=6.03e+4, metrics={'r2': 0.8349}]\n",
            "epoch 52: 100%|██████████| 1366/1366 [00:12<00:00, 107.76it/s, loss=6.01e+4, metrics={'r2': 0.8361}]\n",
            "epoch 53: 100%|██████████| 1366/1366 [00:12<00:00, 107.44it/s, loss=6e+4, metrics={'r2': 0.8365}]\n",
            "epoch 54: 100%|██████████| 1366/1366 [00:12<00:00, 110.04it/s, loss=6.01e+4, metrics={'r2': 0.836}]\n",
            "epoch 55: 100%|██████████| 1366/1366 [00:12<00:00, 107.19it/s, loss=5.98e+4, metrics={'r2': 0.8376}]\n",
            "epoch 56: 100%|██████████| 1366/1366 [00:12<00:00, 106.23it/s, loss=5.95e+4, metrics={'r2': 0.8392}]\n",
            "epoch 57: 100%|██████████| 1366/1366 [00:12<00:00, 107.35it/s, loss=5.95e+4, metrics={'r2': 0.8392}]\n",
            "epoch 58: 100%|██████████| 1366/1366 [00:12<00:00, 106.77it/s, loss=5.94e+4, metrics={'r2': 0.8397}]\n",
            "epoch 59: 100%|██████████| 1366/1366 [00:12<00:00, 107.57it/s, loss=5.93e+4, metrics={'r2': 0.8403}]\n",
            "epoch 60: 100%|██████████| 1366/1366 [00:12<00:00, 108.02it/s, loss=5.9e+4, metrics={'r2': 0.8419}]\n",
            "epoch 61: 100%|██████████| 1366/1366 [00:13<00:00, 104.94it/s, loss=5.92e+4, metrics={'r2': 0.8407}]\n",
            "epoch 62: 100%|██████████| 1366/1366 [00:13<00:00, 102.22it/s, loss=5.89e+4, metrics={'r2': 0.8423}]\n",
            "epoch 63: 100%|██████████| 1366/1366 [00:13<00:00, 104.37it/s, loss=5.87e+4, metrics={'r2': 0.8436}]\n",
            "epoch 64: 100%|██████████| 1366/1366 [00:13<00:00, 102.98it/s, loss=5.87e+4, metrics={'r2': 0.8432}]\n",
            "epoch 65: 100%|██████████| 1366/1366 [00:13<00:00, 101.37it/s, loss=5.87e+4, metrics={'r2': 0.8434}]\n",
            "epoch 66: 100%|██████████| 1366/1366 [00:12<00:00, 105.22it/s, loss=5.86e+4, metrics={'r2': 0.8438}]\n",
            "epoch 67: 100%|██████████| 1366/1366 [00:12<00:00, 105.18it/s, loss=5.84e+4, metrics={'r2': 0.8447}]\n",
            "epoch 68: 100%|██████████| 1366/1366 [00:13<00:00, 104.22it/s, loss=5.82e+4, metrics={'r2': 0.846}]\n",
            "epoch 69: 100%|██████████| 1366/1366 [00:13<00:00, 104.92it/s, loss=5.8e+4, metrics={'r2': 0.8465}]\n",
            "epoch 70: 100%|██████████| 1366/1366 [00:13<00:00, 105.05it/s, loss=5.77e+4, metrics={'r2': 0.8488}]\n",
            "epoch 71: 100%|██████████| 1366/1366 [00:12<00:00, 106.12it/s, loss=5.73e+4, metrics={'r2': 0.8504}]\n",
            "epoch 72: 100%|██████████| 1366/1366 [00:12<00:00, 106.20it/s, loss=5.69e+4, metrics={'r2': 0.8528}]\n",
            "epoch 73: 100%|██████████| 1366/1366 [00:13<00:00, 104.82it/s, loss=5.65e+4, metrics={'r2': 0.8548}]\n",
            "epoch 74: 100%|██████████| 1366/1366 [00:12<00:00, 105.85it/s, loss=5.59e+4, metrics={'r2': 0.8579}]\n",
            "epoch 75: 100%|██████████| 1366/1366 [00:12<00:00, 106.23it/s, loss=5.52e+4, metrics={'r2': 0.8614}]\n",
            "epoch 76: 100%|██████████| 1366/1366 [00:12<00:00, 108.92it/s, loss=5.45e+4, metrics={'r2': 0.8649}]\n",
            "epoch 77: 100%|██████████| 1366/1366 [00:12<00:00, 107.94it/s, loss=5.38e+4, metrics={'r2': 0.8682}]\n",
            "epoch 78: 100%|██████████| 1366/1366 [00:13<00:00, 102.90it/s, loss=5.33e+4, metrics={'r2': 0.8709}]\n",
            "epoch 79: 100%|██████████| 1366/1366 [00:13<00:00, 103.41it/s, loss=5.3e+4, metrics={'r2': 0.8725}]\n",
            "epoch 80: 100%|██████████| 1366/1366 [00:12<00:00, 105.66it/s, loss=5.25e+4, metrics={'r2': 0.8745}]\n",
            "epoch 81: 100%|██████████| 1366/1366 [00:13<00:00, 103.32it/s, loss=5.23e+4, metrics={'r2': 0.8756}]\n",
            "epoch 82: 100%|██████████| 1366/1366 [00:13<00:00, 103.29it/s, loss=5.22e+4, metrics={'r2': 0.8762}]\n",
            "epoch 83: 100%|██████████| 1366/1366 [00:12<00:00, 107.23it/s, loss=5.2e+4, metrics={'r2': 0.8771}]\n",
            "epoch 84: 100%|██████████| 1366/1366 [00:12<00:00, 108.25it/s, loss=5.17e+4, metrics={'r2': 0.8784}]\n",
            "epoch 85: 100%|██████████| 1366/1366 [00:13<00:00, 104.83it/s, loss=5.18e+4, metrics={'r2': 0.878}]\n",
            "epoch 86: 100%|██████████| 1366/1366 [00:13<00:00, 104.70it/s, loss=5.16e+4, metrics={'r2': 0.8787}]\n",
            "epoch 87: 100%|██████████| 1366/1366 [00:13<00:00, 103.06it/s, loss=5.15e+4, metrics={'r2': 0.8793}]\n",
            "epoch 88: 100%|██████████| 1366/1366 [00:12<00:00, 107.30it/s, loss=5.14e+4, metrics={'r2': 0.8796}]\n",
            "epoch 89: 100%|██████████| 1366/1366 [00:12<00:00, 105.11it/s, loss=5.11e+4, metrics={'r2': 0.8812}]\n",
            "epoch 90: 100%|██████████| 1366/1366 [00:13<00:00, 102.43it/s, loss=5.12e+4, metrics={'r2': 0.8807}]\n",
            "epoch 91: 100%|██████████| 1366/1366 [00:12<00:00, 106.11it/s, loss=5.1e+4, metrics={'r2': 0.8814}]\n",
            "epoch 92: 100%|██████████| 1366/1366 [00:12<00:00, 107.14it/s, loss=5.11e+4, metrics={'r2': 0.8815}]\n",
            "epoch 93: 100%|██████████| 1366/1366 [00:13<00:00, 104.16it/s, loss=5.09e+4, metrics={'r2': 0.8817}]\n",
            "epoch 94: 100%|██████████| 1366/1366 [00:12<00:00, 106.42it/s, loss=5.06e+4, metrics={'r2': 0.8834}]\n",
            "epoch 95: 100%|██████████| 1366/1366 [00:13<00:00, 102.42it/s, loss=5.07e+4, metrics={'r2': 0.8831}]\n",
            "epoch 96: 100%|██████████| 1366/1366 [00:13<00:00, 103.59it/s, loss=5.06e+4, metrics={'r2': 0.8835}]\n",
            "epoch 97: 100%|██████████| 1366/1366 [00:12<00:00, 106.08it/s, loss=5.07e+4, metrics={'r2': 0.8831}]\n",
            "epoch 98: 100%|██████████| 1366/1366 [00:13<00:00, 102.08it/s, loss=5.04e+4, metrics={'r2': 0.8843}]\n",
            "epoch 99: 100%|██████████| 1366/1366 [00:13<00:00, 101.55it/s, loss=5.02e+4, metrics={'r2': 0.8854}]\n",
            "epoch 100: 100%|██████████| 1366/1366 [00:13<00:00, 104.34it/s, loss=5.03e+4, metrics={'r2': 0.8846}]\n"
          ]
        }
      ],
      "source": [
        "# TODO: Enter your code here\n",
        "\n",
        "# Reference: https://towardsdatascience.com/pytorch-widedeep-deep-learning-for-tabular-data-9cd1c48eb40d\n",
        "# Target from the train dataset\n",
        "target = df_train['resale_price'].values\n",
        "\n",
        "# taken from my B1\n",
        "# continuous_columns: List[str]: Column names of the numeric fields. Defaults to []\n",
        "# Numeric / Continuous features given: dist_to_nearest_stn, dist_to_dhoby, degree_centrality, eigenvector_centrality, remaining_lease_years, floor_area_sqm\n",
        "continuous_columns = ['dist_to_nearest_stn', 'dist_to_dhoby', 'degree_centrality', 'eigenvector_centrality', 'remaining_lease_years', 'floor_area_sqm']\n",
        "\n",
        "# categorical_columns: List[str]: Column names of the categorical fields to treat differently\n",
        "# Categorical features given: month, town, flat_model_type, storey_range\n",
        "categorical_columns = ['month', 'town', 'flat_model_type', 'storey_range']\n",
        "\n",
        "# create the deeptabular component using the continuous features and the categorical features\n",
        "tab_preprocessor = TabPreprocessor(\n",
        "    cat_embed_cols=categorical_columns, continuous_cols=continuous_columns\n",
        ")\n",
        "\n",
        "# transform the training dataset\n",
        "X_tab = tab_preprocessor.fit_transform(df_train)\n",
        "\n",
        "# Create model with 2 linear layers in the MLP, with 200 and 100 neurons respectively.\n",
        "tab_mlp = TabMlp(\n",
        "    mlp_hidden_dims=[200, 100],\n",
        "    column_idx=tab_preprocessor.column_idx,\n",
        "    cat_embed_input=tab_preprocessor.cat_embed_input,\n",
        "    continuous_cols=continuous_columns\n",
        ")\n",
        "\n",
        "# Create a Trainer for the training of the created TabMlp model with\n",
        "# cost function = root mean squared error (RMSE)\n",
        "# choose R2Score as the metric for next part\n",
        "# num_workers = 0\n",
        "model = WideDeep(deeptabular=tab_mlp)\n",
        "trainer = Trainer(model, cost_function=\"rmse\", metrics=[R2Score], num_workers=0)\n",
        "# Train model for 100 epochs, batch size of 64\n",
        "trainer.fit(\n",
        "    X_tab=X_tab, # Input for the deeptabular model component\n",
        "    target=target,\n",
        "    n_epochs=100, # number of epochs\n",
        "    batch_size=64, # batch size\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V46s-MdM0y5c"
      },
      "source": [
        ">Report the test RMSE and the test R2 value that you obtained."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "KAhAgvMC07g6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "57240fdf-b639-44b9-f0d9-1e54221a9ae3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n",
            "predict: 100%|██████████| 1128/1128 [00:04<00:00, 263.65it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test RMSE: 97868.21495280517\n",
            "Test R2: 0.6653569079110121\n"
          ]
        }
      ],
      "source": [
        "# TODO: Enter your code here\n",
        "import math\n",
        "import sklearn.metrics\n",
        "\n",
        "# Transform the test data\n",
        "X_test_tab = tab_preprocessor.transform(df_test)\n",
        "\n",
        "# Predict the resale prices for the test data\n",
        "y_pred = trainer.predict(X_tab=X_test_tab)\n",
        "\n",
        "print(f\"Test RMSE: {math.sqrt(sklearn.metrics.mean_squared_error(df_test['resale_price'], y_pred))}\")\n",
        "print(f\"Test R2: {sklearn.metrics.r2_score(df_test['resale_price'], y_pred)}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}